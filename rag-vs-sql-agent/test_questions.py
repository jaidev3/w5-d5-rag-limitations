"""
Test Questions for RAG vs SQL Agent Comparison
This module contains various test questions categorized by complexity and type.
"""

from typing import List, Dict, Any

# Test questions categorized by complexity and type
TEST_QUESTIONS = {
    "simple_count": [
        "How many customers are in the database?",
        "How many employees work at the company?",
        "How many tracks are available?",
        "How many artists are in the database?"
    ],
    
    "basic_lookup": [
        "What is the name of customer with ID 1?",
        "Which employee has the title 'Sales Manager'?",
        "What genre is track ID 1?",
        "What is the name of album ID 1?"
    ],
    
    "aggregation": [
        "Which country's customers spent the most money?",
        "What is the average invoice total?",
        "Who is the top-selling artist by number of tracks sold?",
        "What is the most popular genre by number of tracks?"
    ],
    
    "complex_joins": [
        "Which customer has purchased the most tracks?",
        "What are the top 5 most expensive tracks?",
        "Which employee has the most customers assigned to them?",
        "What is the total revenue generated by each genre?"
    ],
    
    "business_intelligence": [
        "What are the monthly sales trends?",
        "Which customers haven't made a purchase in the last year?",
        "What is the customer lifetime value by country?",
        "Which albums have the highest average track price?"
    ],
    
    "exploratory": [
        "Tell me about the database structure",
        "What kind of data is stored in this database?",
        "How are customers and invoices related?",
        "What business insights can I get from this data?"
    ]
}

def get_all_test_questions() -> List[str]:
    """Get all test questions as a flat list"""
    all_questions = []
    for category, questions in TEST_QUESTIONS.items():
        all_questions.extend(questions)
    return all_questions

def get_questions_by_category(category: str) -> List[str]:
    """Get questions for a specific category"""
    return TEST_QUESTIONS.get(category, [])

def get_question_categories() -> List[str]:
    """Get all available question categories"""
    return list(TEST_QUESTIONS.keys())

def get_sample_questions(n: int = 10) -> List[str]:
    """Get a sample of n questions across different categories"""
    all_questions = get_all_test_questions()
    if n >= len(all_questions):
        return all_questions
    
    # Try to get diverse questions from different categories
    sample = []
    questions_per_category = max(1, n // len(TEST_QUESTIONS))
    
    for category, questions in TEST_QUESTIONS.items():
        sample.extend(questions[:questions_per_category])
        if len(sample) >= n:
            break
    
    return sample[:n]

# Expected answer patterns for evaluation
EXPECTED_PATTERNS = {
    "How many customers are in the database?": {
        "type": "count",
        "expected_value": 59,
        "keywords": ["59", "customers", "total"]
    },
    
    "How many employees work at the company?": {
        "type": "count", 
        "expected_value": 8,
        "keywords": ["8", "employees", "work"]
    },
    
    "Which country's customers spent the most money?": {
        "type": "ranking",
        "expected_value": "USA",
        "keywords": ["USA", "United States", "most", "spent"]
    },
    
    "What is the average invoice total?": {
        "type": "aggregation",
        "expected_range": (5.0, 6.0),  # Approximate range
        "keywords": ["average", "invoice", "total"]
    }
}

def evaluate_answer_quality(question: str, answer: str) -> Dict[str, Any]:
    """Evaluate the quality of an answer based on expected patterns"""
    if question not in EXPECTED_PATTERNS:
        return {"evaluated": False, "reason": "No expected pattern defined"}
    
    pattern = EXPECTED_PATTERNS[question]
    evaluation = {
        "evaluated": True,
        "question_type": pattern["type"],
        "keywords_found": [],
        "keywords_missing": [],
        "score": 0.0
    }
    
    # Check for keywords
    answer_lower = answer.lower()
    for keyword in pattern["keywords"]:
        if keyword.lower() in answer_lower:
            evaluation["keywords_found"].append(keyword)
        else:
            evaluation["keywords_missing"].append(keyword)
    
    # Calculate basic score based on keyword presence
    if pattern["keywords"]:
        keyword_score = len(evaluation["keywords_found"]) / len(pattern["keywords"])
        evaluation["score"] = keyword_score
    
    # Additional checks based on question type
    if pattern["type"] == "count" and "expected_value" in pattern:
        if str(pattern["expected_value"]) in answer:
            evaluation["score"] = max(evaluation["score"], 0.9)
            evaluation["exact_match"] = True
        else:
            evaluation["exact_match"] = False
    
    elif pattern["type"] == "ranking" and "expected_value" in pattern:
        if pattern["expected_value"].lower() in answer_lower:
            evaluation["score"] = max(evaluation["score"], 0.9)
            evaluation["correct_ranking"] = True
        else:
            evaluation["correct_ranking"] = False
    
    elif pattern["type"] == "aggregation" and "expected_range" in pattern:
        # Try to extract numeric values from the answer
        import re
        numbers = re.findall(r'\d+\.?\d*', answer)
        if numbers:
            try:
                value = float(numbers[0])
                min_val, max_val = pattern["expected_range"]
                if min_val <= value <= max_val:
                    evaluation["score"] = max(evaluation["score"], 0.8)
                    evaluation["in_range"] = True
                else:
                    evaluation["in_range"] = False
            except ValueError:
                evaluation["in_range"] = False
    
    return evaluation

# Question difficulty levels
DIFFICULTY_LEVELS = {
    "easy": [
        "How many customers are in the database?",
        "How many employees work at the company?", 
        "How many tracks are available?",
        "What is the name of customer with ID 1?"
    ],
    
    "medium": [
        "Which country's customers spent the most money?",
        "What is the average invoice total?",
        "Who is the top-selling artist by number of tracks sold?",
        "Which customer has purchased the most tracks?"
    ],
    
    "hard": [
        "What are the monthly sales trends?",
        "Which customers haven't made a purchase in the last year?",
        "What is the customer lifetime value by country?",
        "Which albums have the highest average track price?"
    ]
}

def get_questions_by_difficulty(difficulty: str) -> List[str]:
    """Get questions filtered by difficulty level"""
    return DIFFICULTY_LEVELS.get(difficulty, [])

def categorize_question_difficulty(question: str) -> str:
    """Determine the difficulty level of a question"""
    for difficulty, questions in DIFFICULTY_LEVELS.items():
        if question in questions:
            return difficulty
    return "unknown"

# Performance benchmarks
PERFORMANCE_BENCHMARKS = {
    "response_time": {
        "excellent": 2.0,    # < 2 seconds
        "good": 5.0,         # 2-5 seconds  
        "acceptable": 10.0,  # 5-10 seconds
        "poor": float('inf') # > 10 seconds
    },
    
    "accuracy": {
        "excellent": 0.9,    # > 90% accuracy
        "good": 0.8,         # 80-90% accuracy
        "acceptable": 0.7,   # 70-80% accuracy
        "poor": 0.0          # < 70% accuracy
    }
}

def benchmark_performance(response_time: float, accuracy: float) -> Dict[str, str]:
    """Benchmark performance metrics against standards"""
    
    def get_rating(value: float, benchmarks: Dict[str, float]) -> str:
        if value <= benchmarks["excellent"]:
            return "excellent"
        elif value <= benchmarks["good"]:
            return "good"
        elif value <= benchmarks["acceptable"]:
            return "acceptable"
        else:
            return "poor"
    
    return {
        "response_time_rating": get_rating(response_time, PERFORMANCE_BENCHMARKS["response_time"]),
        "accuracy_rating": get_rating(accuracy, PERFORMANCE_BENCHMARKS["accuracy"])
    }

if __name__ == "__main__":
    # Example usage
    print("Test Questions by Category:")
    for category in get_question_categories():
        questions = get_questions_by_category(category)
        print(f"\n{category.upper()} ({len(questions)} questions):")
        for i, question in enumerate(questions, 1):
            print(f"  {i}. {question}")
    
    print(f"\nTotal questions: {len(get_all_test_questions())}")
    
    # Sample evaluation
    sample_answer = "There are 59 customers in the database."
    evaluation = evaluate_answer_quality("How many customers are in the database?", sample_answer)
    print(f"\nSample evaluation: {evaluation}") 